# 경윤

***

## 목표

***

- 처음 진행해보는 프로젝트였기 때문에, 프로젝트의 전반적인 프로세스를 경험
    - 가설을 세우고, 해당 가설에 맞게 모델과 loss를 선언해보기
- **첫 번째 목표 달성을 위해**
    - 프로젝트 진행 도중 나오는 모든 내용을 최대한 놓치는 것 없이 다 이해하려고 노력함
    - 새롭게 추가해보고 싶은 기법들을 추가하는 경험 (ex, mixup, motion blur, optimizer 추가)
- **두 번째 목표 달성을 위해**
    - **가설**: swin_base_patch4_window7_224 모델 앞쪽 레이어에서 feature vector를 추출해 대분류 loss를 구하고, 중간에서 feature vector를 추출해 중분류 loss를 구하고, 기존 loss를 소분류 loss로 생각해 3개의 loss를 가중합해서 backpropagation을 적용하면, 바로 500개의 클래스를 구분하는 것보다 적중도가 올라가지 않을까?
    - 가설을 세운 후, 해당 가설에 맞는 모델 선언 및 가중합을 다룬 loss 선언


## 요약

***

* 모델:   
swin_base_patch4_window7_224 -> swin_base_patch4_window7_224 custom 

* Train :   
accumulate_grad_batches(8)

* Augmentation:   
HorizontalFlip(p=0.5), VerticalFlip(p=0.5), Rotate(limit=15), RandomBrightnessContrast(p=0.2)

* 깨달은 점: 많은 클래스를 예측하는 데에 coarse-to-fine 기법이 효과적이었다.


## 자세한 내용

***

### 1. Swin Transformer인 swin_base_patch4_window7_224를 활용한 이유  

Swin Transformer는 윈도우 기반의 local self-attention을 사용하며,   
CNN처럼 계층적 구조를 통해 이미지의 해상도를 점진적으로 줄여가며 특징을 추출하기 때문에, 
ViT와 달리, 이미지의 각 부분에서 로컬 정보를 먼저 학습하고, 이후 글로벌 정보를 윈도우 이동 기법으로 학습하는 방식이므로  
이를 통해 대, 중, 소분류의 다양한 수준의 정보를 점진적으로 추출할 수 있을 거라 판단

### 2. Swin Transformer의 구조

- **Patch Partition and Embedding**  
- **Stage 0** : 2개의 SwinTransformerBlock으로 구성됨
- **Stage 1** : 2개의 SwinTransformerBlock으로 구성됨
-> 블록에서 Window-based Self-Attention, MLP 수행 (패치의 해상도 그대로 유지)
- **Stage 2** : 18개의 SwinTransformerBlock으로 구성됨
- **Stage 3** : 2개의 SwinTransformerBlock으로 구성됨
-> patch merging 단계가 있어 해상도가 줄어듦 (더 높은 수준의 특징 학습 가능)
- **Classification Head**
-> global pooling, FC layer를 통해 분류 작업 수행

### 2. 시도

#### 시도 1
swin_base_patch4_window7_224 모델의 3번째 stage의 1번째 block에서 대분류 feature vector를 뽑고, head 전에서 중분류 feature vector를 뽑아서 loss별 가중치 large_loss_weight=0.1, small_loss_weight=0.2, original_loss_weight=0.7로 실험 진행
    
- **실험 의도 1**: 뒤쪽 레이어에서 feature vector를 구해서 loss를 구하면, 모델의 끝부분에서 더 정제된 feature vector를 사용하여 정확도를 높이려는 의도
    
- **실험 의도 2**: 소분류를 결국 잘하는 게 중요하므로 소분류에 weight를 더 주는 의도
    
- **결과**: validation_acc 0.85092, test_acc 0.866

![image](https://github.com/user-attachments/assets/b0fa46c1-e76c-4fde-9ff7-60e3eb4e9450)

#### 시도 2
모델 개선을 위해 large_loss_weight=0.4, small_loss_weight=0.2, original_loss_weight=0.4로 실험 진행
    
- **실험 의도**: 대분류와 소분류에 동일한 가중치를 부여하면, 중분류 성능이 조금 떨어지더라도 대분류와 소분류에서의 학습이 더 강화될 것
    
- **결과**: validation_acc 0.84526

#### 시도 3
validation_acc를 높이기 위해, large_loss_weight=0.5, small_loss_weight=0.3, original_loss_weight=0.2로 실험 진행
    
- **실험 의도**: 대분류 - 중분류 - 소분류 순으로 weight를 주어, 큰 분류에서 분류가 잘 되면 뒤에서도 자연스레 잘 분류되지 않을까 하는 의도
    
- **결과**: validation_acc 0.84426, test_acc 0.857

![image](https://github.com/user-attachments/assets/b9795c2c-a31e-4ead-82cc-54bc9366b141)

#### 시도 4
시도 3과 같은 조건에 Stratified K-Fold를 fold=5로 진행
    
- **실험 의도**: 대분류, 중분류, 소분류 문제에서 클래스 비율이 중요하다고 판단했고, Stratified K-Fold가 매우 유용할 것으로 생각

- **결과**: validation_acc 0.85586, test_acc 0.902

![image](https://github.com/user-attachments/assets/6b223ed9-c7e1-4ac3-b974-d9c6163958c1)

#### 시도 5
시도 3과 같은 조건에 Stratified K-Fold를 fold=10으로 진행
    
- **실험 의도**: 데이터 수가 적을 때 fold 수를 세분화하여 모델이 더 다양한 데이터 분포를 학습할 기회를 제공해서 성능을 향상시킬 수 있는 의도
    
- **결과**: validation_acc 0.86352, test_acc 0.904

![image](https://github.com/user-attachments/assets/0dee0eac-996b-4aa2-883a-9be01e2b5c6d)

#### 시도 6
swin_base_patch4_window7_224 모델의 2번째 Stage의 17번째 block에서 대분류 feature vector를 뽑고, 3번째 Stage의 1번째 block에서 중분류 feature vector를 뽑아서 loss별 가중치 large_loss_weight=0.5, small_loss_weight=0.3, original_loss_weight=0.2로 실험 진행 (Stratified K-Fold fold=5)
    
- **실험 의도**: 모델의 앞쪽 레이어는 더 일반적인 특징을 추출하므로, 앞쪽 레이어에서 추출된 덜 세부적인 특징을 사용해 대분류를 분류하면 성능이 오를 것이라는 의도
    
- **결과**: validation_acc 0.84421, test_acc 0.891

![image](https://github.com/user-attachments/assets/ad8fb711-ef9e-43c6-850c-72314f5b83d2)

***

### 3. 한계 및 아쉬운 점

- **한계**: 시도 6에서 레이어를 바꾼 부분이 같은 조건의 시도 4와 비교했을 때 test_acc가 낮게 나왔는데, 앞쪽 레이어에서 대분류 feature vector를 빼오면 성능이 더 오를 거라는 예상과 달라서 한계에 부딪혔다.

- **아쉬웠던 점**: 시간이 부족해 체계적인 실험을 하지 못 한 점이 아쉽다. 돌이켜 생각해보면, 시도 2에서 test 제출을 해보고, 해당 값과 시도 3의 값을 비교 후 다음 단계로 실험이 넘어갔어야 했는데, 바로 시도 3의 결과를 더 발전시켜서 시도 2의 결과에 대한 고찰이 부족했다. 더하여 여러 cutmix, sweep 등을 시간 부족으로 시도해보지 못한 점도 아쉽다.

***

### 4. 한계/교훈을 바탕으로 다음 프로젝트에서 시도해보고 싶은 점
먼저, 가설을 세우고 그에 따라 실험을 진행하는 것이 중요하다는 것을 깨달았고, 실험 변수를 하나씩 조정하면서 보다 체계적으로 접근해야겠다는 생각을 하게 되었다. 또한, 실험 내용을 꼼꼼히 기록하는 것이 얼마나 중요한지도 배웠다. 더불어, 프로젝트 기간 동안 시간 관리의 중요성을 느꼈고, 팀원 간 코드 공유를 통해 시간을 절약할 수 있는 방법을 팀원분들께 제안드렸다.