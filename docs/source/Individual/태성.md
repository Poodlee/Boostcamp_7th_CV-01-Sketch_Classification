# 태성

  

## 목표

첫 번째 목표: 프로젝트 형식 맞추기.

두 번째 목표: 데이터 기반 가설 설정 및 증명 과정 경험.

  

## 요약

- 강한 Augmentation 적용으로 성능 저하 확인.
- 원본 이미지와 강한 Augmentation 이미지를 합쳐 학습했으나 성능에 차이 없거나, 하락
- softmax 값을 통한 Noise 제거 시도했으나 성공적이지 않음.
- Cosine Similarity를 통한 중복 제거는 효과적이었으나 성능에 큰 변화 없음.
- CoCa 및 CLIP Pretrained 모델을 사용한 학습에서 상대적으로 높은 성능 확인.

  

## 자세한 내용

- 강한 Augmentation
	- sketch data의 특성 상 강한 augmentation도 sementic을 해치지 않을 것이라는 가설
	- Sketch 데이터에 ElasticTransform, GridDistortion, Afine을 적용했으나 성능이 떨어짐.
	- Fine-grained 클래스가 포함되어 있어, 세밀한 정보가 중요했을 것이라고 판단.

- Origin 이미지와 Augmentation 이미지 Concat
	- Training과 Test 시 Canny와 같은 Augmentation을 추가적으로 정보로 사용.
	- 모델을 먼저 학습하고 Feature를 Concat하여 fc layer를 통과한 성능을 확인했으나 성능 저하.
	- 여러 Hyperparameter가 복잡하게 얽혀 조정 부족, 데이터 부족으로 늘어난 분포 커버 불가능.

- K-fold로 Softmax Noise 제거
	- Target Softmax Value와 Maximum Softmax Value를 사용해 Noise 제거를 시도했으나 실패.
	- 멘토님으로 부터 Sigmoid 활용 제안, fine grained class가 있어 softmax의 normalization 부분이 값이 작게 만든거 같다

- Cosine Similarity를 통한 중복 제거
	- Threshold 0.97로 중복 제거, 사용된 모델의 성능은 val acc 80%.
	- 중복 제거는 성공적이었으나 성능에는 큰 변화 없음.

- CoCa 및 CLIP Pretrained 모델 사용
	- FC Layer만 학습.
	- Imagenet이 아닌 lion2b dataset으로 Pretrained되어 더 나은 성능을 기대.
	- 기존 보다는 높은 성능 
	- 다른 조의 image net으로 학습한 EVA 모델에 비해 성능은 낮았기에 데이터 가설은 불충분 결론

## 후기 및 교훈
- 긴 Iteration 및 Hyperparameter 조정 부족으로 성능 개선에 어려움.
- Test Set의 분포를 더 고려했어야 함.
- CUDA 설치로 인한 서버 문제를 분석하지 못한 점이 아쉬움.

## 다음 프로젝트에서 시도할 것
- 빠른 Iteration 개발.
- 데이터 기반 가설 설정.
- Hyperparameter 튜닝에 집중.