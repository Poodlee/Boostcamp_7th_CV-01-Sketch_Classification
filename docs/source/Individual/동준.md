# 동준

## 목표

* 첫 딥러닝 프로젝트 성공적으로 마무리 (with pytorch lightning)
* Ensemble을 통한 성능 향상 

## 요약

* 모델: ResNet,ResNext,Swin_transformer,Ensemble(vit_clip,swin,convnextv2)
* Augmentation: Canny,Morhphology,Gaussian blur,Motion blur,RandAugment,Autoaugment
* 깨달은 점: 모듈의 전체적인 구성 파악, 기존 오픈 소스 모델 수정 방법, Ensemble 구현 방법 


## 자세한 내용

### Result

단일 모델
<p align="center">
<img src="https://github.com/user-attachments/assets/b258bb23-6b84-41c3-aa15-5f1edb3fdffb" alt="image" width="700"/>
</p>

Ensemble(swin trnasformer + resnext) Result

<p align="center">
<img src="https://github.com/user-attachments/assets/e81a87ce-9a3d-4c48-a352-74c7eee7c838" alt="image" width="700"/>
</p>

<p align="center">
<img src="https://github.com/user-attachments/assets/b2fd77bc-1d8c-4bb0-a004-468e3fadc51a" alt="image" width="700"/>
</p>


Ensemble(swin transformer + vit_clip + convnextv2)

<p align="center">
<img src="https://github.com/user-attachments/assets/e81f2a92-6e42-4daa-b10d-a97e4a6ad3ce" alt="image" width="700"/>
</p>

### 결과 분석

결과를 종합해보면, 단일 모델에서 앙상블로 전환할수록, 그리고 앙상블에 더 많은 모델을 포함할수록 성능이 향상되는 경향을 확인할 수 있습니다. 이는 앙상블 기법이 다양한 모델의 예측을 결합해 개별 모델의 약점을 보완하고, 보다 강력하고 안정적인 성능을 이끌어내는 일반적인 특성과 일치합니다.
<p align="center">
<img src="https://github.com/user-attachments/assets/2e6d6e2c-bf65-4ce9-8f58-bddc43ed75da" alt="image" width="500"/>
</p>

<p align="center">
<img src="https://github.com/user-attachments/assets/3e458438-e66f-4811-b68a-72b57455374c" alt="image" width="500"/>
</p>

<p align="center">
<img src="https://github.com/user-attachments/assets/6b6009ee-8389-4014-976e-48c2b4c0763a" alt="image" width="500"/>
</p>
그럼에도 불구하고 첨부한 사진에서 알 수 있듯이, 여전히 validation accuracy가 test accuracy에 비해 10% 이상으로 지나치게 높게 나타나고 있습니다. 이는 과적합을 의미하며, 모델이 validation 데이터에 과도하게 적응하고 있음을 의미할 수 있습니다.
과적합을 해결하기 위해 stratified K-Fold Cross-Validation을 적용하고, 데이터 증강(Augmentation)도 변경 및 제거해 보았지만, 이러한 현상은 여전히 남아 있었습니다.

---

## Procedure 

처음에는 클래스당 데이터 개수가 30개밖에 없어, 데이터의 표현력을 높여야 한다고 생각했습니다. 그래서 Canny, Morphology, Gaussian Noise, Motion Blur 등의 오프라인 증강(offline augmentation)을 여러 경우로 나누어 수행했습니다. 그러나 모든 이미지에 단순히 오프라인 증강한 데이터를 원본에 더하는 형식으로 적용한 결과, 전체 이미지의 양이 원본의 n배로 늘어나게 됐습니다. 이때 train과 validation 세트로 나누어 학습하는 과정에서 기존 데이터와 증강된 데이터가 각각 두 세트에 섞이면서 과적합이 심각하게 발생했습니다. (이때, 과적합을 방지하려면 원본 데이터와 증강된 데이터가 함께 묶여야 할 것 같습니다.)

또한 데이터를 전체적으로 검토한 결과, 동일 이미지에 대한 다중 클래스 할당이 많고, 클래스와 무관한 이미지도 다수 포함되어 있어 성능에 부정적인 영향을 미쳤음을 확인했습니다. 이에 아래 두 가지 실험을 진행했습니다.

### 주요 실험 내용

1. **동일 이미지에 대한 다중 클래스 할당 및 클래스와 무관한 이미지 제거**
   - Structural Similarity (SSIM)와 Cosine Similarity를 계산하여 유사한 이미지를 제거한 후 직접 검수하여 데이터셋을 정제하였습니다.
   
2. **Offline Augmentation의 효과 파악**
   - 여러 가지 Augmentation 기법을 적용해보고 원본 데이터와 비교하여 결과 차이를 분석했습니다.

먼저 진행한 실험은 동일 이미지에 대한 다중 클래스 할당과 클래스와 무관한 이미지의 제거입니다. 데이터셋을 검토한 결과, 동일한 강아지 사진이 세로 및 가로로 뒤집히거나 단순 중복된 형태로 여러 클래스에 반복되어 있는 것을 확인했습니다. 이를 제거하기 위해 크게 두 가지 단계로 접근했습니다.
첫 번째 단계는 클래스 내 동일 이미지의 재사용 문제를 해결하는 것이었습니다. 이를 위해 각 클래스의 이미지들 간에 Structural Similarity (SSIM)를 계산하여 유사도를 측정했습니다. SSIM의 측정 기준은 다음과 같습니다:

- Luminance: 슬라이딩 윈도우를 사용하여 픽셀의 평균값을 계산
- Contrast: 슬라이딩 윈도우를 사용하여 이미지의 표준편차를 계산
- Structure: 두 이미지 사이의 공분산을 측정
이러한 기준을 통해 유사도가 95% 이상인 이미지를 제거하였습니다.

다음 단계는 동일 이미지에 대한 다중 클래스 할당 문제를 해결하는 것이었습니다. 앞서 제거한 이미지를 토대로 전체 이미지에 대해 Cosine Similarity를 계산하여 유사도가 99% 이상인 이미지를 추가로 제거했습니다. 이 과정을 통해 데이터 개수가 원본 데이터의 약 15,000개에서 11,000개로 줄어들었습니다. 그러나 이때 클래스별 이미지 개수가 10개 이하인 클래스가 8개 정도, 그중에서도 1~2개인 클래스가 5개나 발생하여, 검수 이전과 비교할 테스트를 바로 진행하지 못하고 offline augmentation과 함께 수행했습니다.

<p align="center">
<img src="https://github.com/user-attachments/assets/ad8bc65c-ebbf-44b3-a3c2-73edc462737d" alt="image" width="700"/>
</p>

두 번째 실험은 Offline Augmentation의 효과 파악이었습니다. 중복 이미지를 제거한 데이터셋에 대해 다양한 Offline Augmentation을 적용해 보았는데, 결과적으로 Augmentation을 적용한 모델의 성능이 오히려 떨어졌습니다. 개인적으로, 원본 데이터에서 너무 많은 이미지를 제거하여 모델의 표현력이 감소한 것이 아닌가 생각합니다. 하지만 이는 가정에 불과하며 정확한 원인은 일일이 검토해야 알 수 있을 것입니다. 다만 모든 경우를 테스트하기에는 시간이 너무 부족하여 추가 검토를 진행하지 못했습니다.
또한, 이 과정에서 여전히 과적합 문제가 지속되었습니다. 이를 해결하기 위해 찾아보던 중 Stratified K-Fold와 Ensemble Stacking 기법을 알게 되었고, 이를 적용해 보았습니다.

---

### Ensemble에 사용한 모델

1. **ViT_giant_clip:** 큰 데이터셋으로 pre-trained된 모델.
2. **Swin Transformer:** 일반적으로 성능이 뛰어나다고 하는 Transformer 모델.
3. **ConvNeXtV2_huge:** CNN 기반의 모델.
각각에 대한 간단한 설명은 아래와 같습니다. 처음으로 임하는 것이다 보니 각 모델의 특징에 대해 자세히 알아보진 못했습니다.


#### Swin Transformer
ViT의 한계를 극복하고 CNN의 이점까지 반영한 모델로, 지역적 정보와 전역적 정보를 동시에 학습할 수 있습니다. ‘Shifted Window’ 메커니즘을 도입해 연산 효율을 높였고 다양한 이미지 인식 작업에서 높은 성능을 보여줍니다. 

<p align="center">
<img src="https://github.com/user-attachments/assets/6d311142-412d-4e82-a026-5344c73356df" alt="image" width="500"/>
</p>
전체적인 구조를 보면 CNN과 동일하게 가져가기 위해 이미지를 patch로 나누고 작은 윈도우 단위로 나누어 Self-Attention을 수행한다. (일반적인 ViT는 전체 이미이지에 대해 한다는 점이 다르다.) 또한 이때 Shifted Window 전략을 사용하여 작은 윈도우의 self-attention만 하지 않고 윈도우의 위치를 변경시키면서 인접한 윈도우 간의 정보가 교환되어 결국에는 전체적인 맥락을 포착할 수 있다.  또한 이러한 Stage가 끝나게 되면 feature map을 병합하게 되면서 그 크기는 줄이고 channel수를 늘리는 전략을 수행했다. 그리고 최종적으로 Head를 통해 이미지 분류를 수행한다.


#### ConvNeXtV2

<p align="center">
<img src="https://github.com/user-attachments/assets/630393e9-0602-435a-9964-de942138109e" alt="image" width="500"/>
</p>

ConvNeXtV2는 기존 CNN(Convolutional Neural Network) 아키텍처를 현대적으로 재해석한 모델로, Vision Transformer의 아이디어를 적극 활용하여 뛰어난 성능을 보여줍니다. ConvNeXtV2는 ConvNeXt의 후속 버전으로, 단순한 CNN의 구조를 유지하면서도 최신 트렌드와 다양한 개선 사항을 통합해 한층 발전된 모델을 제시합니다. 먼저, ConvNeXtV2는 일반적인 CNN보다 훨씬 큰 커널 사이즈를 사용해 더 넓은 수용 영역을 확보함으로써 이미지의 전역적인 특성을 효과적으로 포착합니다. 이는 Vision Transformer가 이미지의 전반적인 정보를 처리하는 방식을 받아들인 결과로, 기존 CNN이 주로 국소적인 특징에 집중하는 한계를 극복하는 데 도움을 줍니다. 또한, ConvNeXtV2는 Layer Normalization을 도입해 일반적인 CNN에서 사용하던 Batch Normalization을 대체함으로써 학습의 안정성을 높이고, 다양한 크기의 데이터에 적응하는 능력을 강화했습니다. 이뿐만 아니라 ConvNeXtV2는 각 블록에서 더 많은 정보를 전달할 수 있도록 Residual Block을 개선하고, MLP 레이어와 결합하여 풍부한 특징을 학습합니다.

특히 `Depthwise Convolution`을 활용하여 각 채널마다 독립적으로 특징을 추출하는 방식을 채택하고 있습니다. `Depthwise Convolution`은 일반적인 합성곱 연산과 달리, 각 입력 채널에 별도의 필터를 적용하여 독립적으로 특징을 추출합니다. 이 방식은 연산량을 줄이면서도 모델이 각 채널의 세부적인 패턴을 더 잘 파악할 수 있게 해줍니다. 그 결과, ConvNeXtV2는 더 가벼운 연산으로도 높은 성능을 낼 수 있으며, 정보의 손실 없이 다양한 이미지의 복잡한 패턴을 학습할 수 있습니다. 게다가 ConvNeXtV2는 대규모 데이터셋에서 효율적으로 학습하기 위해 훈련 방식을 최적화했으며, Huge 버전은 수많은 파라미터와 깊은 네트워크 구조를 바탕으로 복잡하고 세밀한 이미지 패턴을 처리할 수 있습니다. 이러한 모든 개선 사항들은 ConvNeXtV2를 단순히 CNN의 업그레이드 버전이 아닌, Transformer의 강점을 아우르는 하이브리드 모델로 만들어주며, 다양한 비전 작업에서 최고의 성능을 발휘할 수 있게 합니다. 이때 학습 효율 재고 및 더 좋은 특징 표현 학습하기 위해 Masked Autoencoder(MAE) 방식을 활용합니다.

최종적으로 3개의 모델을 합쳐 아래와 같은 Ensemble 구조를 구현했습니다. 
<p align="center">
<img src="https://github.com/user-attachments/assets/92ba0fae-051d-4722-96fa-6a1d2f380eed" alt="image" width="500"/>
</p>

---

### 회고 및 앞으로의 계획

이번 프로젝트에서는 딥러닝 프로젝트의 구조를 이해하는 데 많은 시간을 할애하다 보니, 성능이 좋다고 알려진 학습 구조나 모델을 그대로 사용하는 데 그쳤습니다. 이를 바탕으로 다음 프로젝트(Object Detection)에서는 보다 주도적으로 모델을 선택하고, 단순히 SOTA(State-Of-The-Art) 모델을 사용하는 대신, 해당 모델의 레이어를 직접 구현하여 하나씩 테스트해볼 계획입니다.

또한, 단순히 테스트 점수를 내는 것에 그치지 않고, 실제로 영상에서 Object Detection을 수행하면서 필요한 백본을 수정해 나가겠습니다. 이를 통해 실시간 사용이 가능한 30fps를 달성하고, 성능을 최대로 높이는 것을 목표로 하고자 합니다.
