# 동준

## 목표

* 첫 딥러닝 프로젝트 성공적으로 마무리 (with pytorch lightning)
* Ensemble을 통한 성능 향상 

## 요약

* 모델: ResNet,ResNext,Swin_transformer,Ensemble(vit_clip,swin,convnextv2)
* Augmentation: Canny,Morhphology,Gaussian blur,Motion blur,RandAugment,Autoaugment
* 깨달은 점: 모듈의 전체적인 구성 파악, 기존 오픈 소스 모델 수정 방법, Ensemble 구현 방법 


## 자세한 내용

### Result

단일 모델
<p align="center">
<img src="https://github.com/user-attachments/assets/b258bb23-6b84-41c3-aa15-5f1edb3fdffb" alt="image" width="700"/>
</p>

Ensemble(swin trnasformer + resnext) Result

<p align="center">
<img src="https://github.com/user-attachments/assets/e81a87ce-9a3d-4c48-a352-74c7eee7c838" alt="image" width="700"/>
</p>

<p align="center">
<img src="https://github.com/user-attachments/assets/b2fd77bc-1d8c-4bb0-a004-468e3fadc51a" alt="image" width="700"/>
</p>


Ensemble(swin transformer + vit_clip + convnextv2)

<p align="center">
<img src="https://github.com/user-attachments/assets/e81f2a92-6e42-4daa-b10d-a97e4a6ad3ce" alt="image" width="700"/>
</p>

### 결과 분석

결과를 종합해보면, 단일 모델에서 앙상블로 전환할수록, 그리고 앙상블에 더 많은 모델을 포함할수록 성능이 향상되는 경향을 확인할 수 있습니다. 이는 앙상블 기법이 다양한 모델의 예측을 결합해 개별 모델의 약점을 보완하고, 보다 강력하고 안정적인 성능을 이끌어내는 일반적인 특성과 일치합니다.
<p align="center">
<img src="https://github.com/user-attachments/assets/2e6d6e2c-bf65-4ce9-8f58-bddc43ed75da" alt="image" width="500"/>
</p>

<p align="center">
<img src="https://github.com/user-attachments/assets/3e458438-e66f-4811-b68a-72b57455374c" alt="image" width="500"/>
</p>

<p align="center">
<img src="https://github.com/user-attachments/assets/6b6009ee-8389-4014-976e-48c2b4c0763a" alt="image" width="500"/>
</p>
그럼에도 불구하고 첨부한 사진에서 알 수 있듯이, 여전히 validation accuracy가 test accuracy에 비해 10% 이상으로 지나치게 높게 나타나고 있습니다. 이는 과적합을 의미하며, 모델이 validation 데이터에 과도하게 적응하고 있음을 의미할 수 있습니다.
과적합을 해결하기 위해 stratified K-Fold Cross-Validation을 적용하고, 데이터 증강(Augmentation)도 변경 및 제거해 보았지만, 이러한 현상은 여전히 남아 있었습니다.

---

## Procedure 

처음에는 클래스당 데이터 개수가 30개 밖에 없기에, 데이터당 표현력을 늘려야겠다 생각하여 offline augmentation을 수행했습니다. 하지만 offline augmentation을 수행하면서 증강된 데이터가 큰 확률을 통해 같이 훈련 데이터로 로드하거나 같이 validation 데이터로 로드해야 합니다. 하지만 처음이다 보니 확률적으로 나뉘지 않을까 해서 그냥 8대2로 나눴었습니다. 하지만 동일한 이미지에 대해 훈련 데이터와 validation 데이터로 들어가니 과적합이 심하게 발생했다. validation accuracy가 거의 98에 이르렀는데 실제 test 제출 시에 0.835와 같이 과적합이 심하게 발생한 것을 알게 됐습니다.

그래서 데이터를 전체적으로 다시 보니 동일 이미지에 대한 다중 클래스 할당도 많고 거기다가 offline augmentation이 train과 validation으로 나눠 들어가니 엄청난 과적합을 보였던 것으로 파악했습니다. 그래서 가장 먼저 2가지 실험을 진행했습니다.

### 주요 실험 내용

1. **동일 이미지에 대한 다중 클래스 할당 제거**
   - 직접 검수하여 적절히 제거.
   
2. **Offline Augmentation 제거**
   - Augmentation을 제거해 train이나 validation 데이터로 중복되지 않도록 함.
   
테스트 결과 동일 이미지에 대한 다중 클래스를 제거하고 유사한 이미지를 similarity를 통해 제거했는데 오히려 결과가 떨어지는 것을 확인했습니다. 개인적인 견해로는 수평 수직으로 offline augmentation된 이미지까지 거의 다 제거를 했는데 이부분을 통해 성능이 떨어진 것 같습니다. 하지만 단순히 가정일 뿐 정확한 것은 일일이 확인해봐야 알 수 있을 것 같습니다.  근데 그 정도를 일일이 테스트 하기에는 시간이 너무나도 부족해서 수행하지 못했습니다.  

그래서 최종적으로 기존에 offline-augmentation(canny, gaussian blur, morphology 등)을 제외하고 원본 데이터를 이용해서 수행했다. 그 결과 기존에 0.98정도의 validation은 떨어지고 test accuracy는 기존과 유사하여 validation score에 대한 신뢰가 조금은 다시 생긴 것 같았습니다. 그러나 여전히 과적합의 늪에 빠졌었습니다… 그래서 찾아 보던 중 과적합을 해결하기 위해 `Stratified K-Fold`와 `Ensemble Stacking` 수단을 알게 됐습니다. 는 fold마다 클래스 비율을 유지 시켜 데이터의 각 부분을 테스트할 수 있어 오버피팅을 방지하고 ensemble의 경우 성능이 뛰어난 모델들을 합하면 더 좋은 성능을 낼 수 있다는 말에 수행해봤습니다. 

---

### Ensemble에 사용한 모델

1. **ViT_giant_clip:** 큰 데이터셋으로 pre-trained된 모델.
2. **Swin Transformer:** 일반적으로 성능이 뛰어나다고 하는 Transformer 모델.
3. **ConvNeXtV2_huge:** CNN 기반의 모델.
각각에 대한 간단한 설명은 아래와 같습니다. 처음으로 임하는 것이다 보니 각 모델의 특징에 대해 자세히 알아보진 못했습니다.


#### Swin Transformer
ViT의 한계를 극복하고 CNN의 이점까지 반영한 모델로, 지역적 정보와 전역적 정보를 동시에 학습할 수 있습니다. ‘Shifted Window’ 메커니즘을 도입해 연산 효율을 높였고 다양한 이미지 인식 작업에서 높은 성능을 보여줍니다. 

<p align="center">
<img src="https://github.com/user-attachments/assets/6d311142-412d-4e82-a026-5344c73356df" alt="image" width="500"/>
</p>
전체적인 구조를 보면 CNN과 동일하게 가져가기 위해 이미지를 patch로 나누고 작은 윈도우 단위로 나누어 Self-Attention을 수행한다. (일반적인 ViT는 전체 이미이지에 대해 한다는 점이 다르다.) 또한 이때 Shifted Window 전략을 사용하여 작은 윈도우의 self-attention만 하지 않고 윈도우의 위치를 변경시키면서 인접한 윈도우 간의 정보가 교환되어 결국에는 전체적인 맥락을 포착할 수 있다.  또한 이러한 Stage가 끝나게 되면 feature map을 병합하게 되면서 그 크기는 줄이고 channel수를 늘리는 전략을 수행했다. 그리고 최종적으로 Head를 통해 이미지 분류를 수행한다.


#### ConvNeXtV2

<p align="center">
<img src="https://github.com/user-attachments/assets/630393e9-0602-435a-9964-de942138109e" alt="image" width="500"/>
</p>

ConvNeXtV2는 기존 CNN(Convolutional Neural Network) 아키텍처를 현대적으로 재해석한 모델로, Vision Transformer의 아이디어를 적극 활용하여 뛰어난 성능을 보여줍니다. ConvNeXtV2는 ConvNeXt의 후속 버전으로, 단순한 CNN의 구조를 유지하면서도 최신 트렌드와 다양한 개선 사항을 통합해 한층 발전된 모델을 제시합니다. 먼저, ConvNeXtV2는 일반적인 CNN보다 훨씬 큰 커널 사이즈를 사용해 더 넓은 수용 영역을 확보함으로써 이미지의 전역적인 특성을 효과적으로 포착합니다. 이는 Vision Transformer가 이미지의 전반적인 정보를 처리하는 방식을 받아들인 결과로, 기존 CNN이 주로 국소적인 특징에 집중하는 한계를 극복하는 데 도움을 줍니다. 또한, ConvNeXtV2는 Layer Normalization을 도입해 일반적인 CNN에서 사용하던 Batch Normalization을 대체함으로써 학습의 안정성을 높이고, 다양한 크기의 데이터에 적응하는 능력을 강화했습니다. 이뿐만 아니라 ConvNeXtV2는 각 블록에서 더 많은 정보를 전달할 수 있도록 Residual Block을 개선하고, MLP 레이어와 결합하여 풍부한 특징을 학습합니다.

특히 `Depthwise Convolution`을 활용하여 각 채널마다 독립적으로 특징을 추출하는 방식을 채택하고 있습니다. `Depthwise Convolution`은 일반적인 합성곱 연산과 달리, 각 입력 채널에 별도의 필터를 적용하여 독립적으로 특징을 추출합니다. 이 방식은 연산량을 줄이면서도 모델이 각 채널의 세부적인 패턴을 더 잘 파악할 수 있게 해줍니다. 그 결과, ConvNeXtV2는 더 가벼운 연산으로도 높은 성능을 낼 수 있으며, 정보의 손실 없이 다양한 이미지의 복잡한 패턴을 학습할 수 있습니다. 게다가 ConvNeXtV2는 대규모 데이터셋에서 효율적으로 학습하기 위해 훈련 방식을 최적화했으며, Huge 버전은 수많은 파라미터와 깊은 네트워크 구조를 바탕으로 복잡하고 세밀한 이미지 패턴을 처리할 수 있습니다. 이러한 모든 개선 사항들은 ConvNeXtV2를 단순히 CNN의 업그레이드 버전이 아닌, Transformer의 강점을 아우르는 하이브리드 모델로 만들어주며, 다양한 비전 작업에서 최고의 성능을 발휘할 수 있게 합니다.

이때 학습 효율 재고 및 더 좋은 특징 표현 학습하기 위해 Masked Autoencoder(MAE) 방식을 활용한다.

최종적으로 3개의 모델을 합쳐 아래와 같은 Ensemble 구조를 구현했습니다. 
<p align="center">
<img src="https://github.com/user-attachments/assets/92ba0fae-051d-4722-96fa-6a1d2f380eed" alt="image" width="500"/>
</p>

---

### 회고 및 앞으로의 계획

기존에는 `vit_clip`과 `swin transformer`만으로 앙상블을 진행했으나, 더 많은 모델을 시도해보고 싶다는 생각에 CNN 기반 모델인 `ConvNeXtV2`를 추가하여 총 3개의 앙상블을 구현했습니다. 하지만 각 레이어에 따른 결과 변화에 대한 인식이 아직 부족해 아쉬움이 남습니다. 앞으로 이 부분에 대해 더 깊이 공부해야겠다고 다짐했습니다. 또한 학습 시간이 18시간 정도로 상당히 오래 걸리기 때문에, 다음 프로젝트에서는 성능을 유지하면서 학습 시간을 단축하는 방법을 모색해볼 생각입니다.
